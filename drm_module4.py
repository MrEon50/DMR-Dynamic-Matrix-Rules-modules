
"""drm_module4.py (extended)
Added features:
 - RuleGenerator: latent-generator of rules with lightweight train-on-replay capability (numpy-based)
 - Optional numpy acceleration for vectorized multiplicative_update
 - Generator-registering so replay samples can train generator
 - Backwards-compatible API additions: DRMSystem.generate_rules, Rule.latent_z
"""

import math
import random
from typing import Dict, Optional, List, Tuple
from collections import deque

# optional numpy acceleration
try:
    import numpy as np
    HAS_NUMPY = True
except Exception:
    np = None
    HAS_NUMPY = False

EPS = 1e-9

class Rule:
    def __init__(self, name: str, init_weight: float = 1.0, init_mean: float = 0.5, init_var: float = 0.25, latent_z: Optional[List[float]] = None):
        self.name = name
        self.weight = float(init_weight)
        self.post_mean = float(init_mean)
        self.post_var = float(init_var)
        self.observations = 0
        self.usage_count = 0
        self.is_new = True
        self.quarantined = False
        # store latent vector if generated by a generator
        self.latent_z = latent_z

    def update_posterior(self, reward: Optional[float], obs_var: float = 0.05):
        if reward is None:
            return
        self.observations += 1
        self.usage_count += 1
        self.is_new = False
        prec_prior = 1.0 / max(EPS, self.post_var)
        prec_like = 1.0 / max(EPS, obs_var)
        post_var = 1.0 / (prec_prior + prec_like)
        post_mean = post_var * (self.post_mean * prec_prior + reward * prec_like)
        self.post_mean = float(post_mean)
        self.post_var = float(post_var)

    def sample_posterior(self) -> float:
        sigma = math.sqrt(max(EPS, self.post_var))
        return random.gauss(self.post_mean, sigma)

    def post_prob_below(self, threshold: float) -> float:
        if self.post_var <= 0:
            return 1.0 if self.post_mean < threshold else 0.0
        z = (threshold - self.post_mean) / math.sqrt(self.post_var)
        cdf = 0.5 * (1 + math.erf(z / math.sqrt(2)))
        return float(max(0.0, min(1.0, cdf)))

    def to_dict(self):
        return {
            "name": self.name,
            "weight": self.weight,
            "post_mean": self.post_mean,
            "post_var": self.post_var,
            "observations": self.observations,
            "usage_count": self.usage_count,
            "quarantined": self.quarantined,
            "latent_z": None if self.latent_z is None else (list(self.latent_z) if isinstance(self.latent_z, (list, tuple)) else None),
        }


class ReplayBuffer:
    def __init__(self, capacity: int = 10000):
        self.buf = deque(maxlen=capacity)

    def add(self, entry: Tuple[str, float]):
        self.buf.append(entry)

    def sample(self, k: int) -> List[Tuple[str, float]]:
        if not self.buf:
            return []
        k = min(k, len(self.buf))
        return random.sample(list(self.buf), k)


class RuleGenerator:
    """Simple latent linear generator mapping z -> (mean,var,weight).
    Uses numpy if available for internal parameters. 
    Can be trained on replayed samples that map rules' latent z to observed rewards.
    """
    def __init__(self, latent_dim: int = 8, seed: Optional[int] = None):
        self.latent_dim = int(latent_dim)
        if seed is not None:
            random.seed(seed)
        # parameters for mapping to mean: mean = sigmoid(w_mean @ z + b_mean)
        if HAS_NUMPY:
            self.W_mean = np.random.normal(scale=0.5, size=(1, self.latent_dim))
            self.b_mean = np.random.normal(scale=0.1, size=(1,))
        else:
            self.W_mean = [[random.gauss(0,0.5) for _ in range(self.latent_dim)]]
            self.b_mean = [random.gauss(0,0.1)]
        # small fixed var baseline mapping
        self.base_var = 0.05

        # registry: maps rule_name -> latent_z for generated rules
        self.registry: Dict[str, List[float]] = {}

    def _sigmoid(self, x):
        return 1.0 / (1.0 + math.exp(-x))

    def _to_numpy(self, z):
        if HAS_NUMPY:
            return np.asarray(z).reshape(self.latent_dim)
        else:
            # keep as list
            return z

    def sample_latent(self):
        if HAS_NUMPY:
            return np.random.normal(size=(self.latent_dim,))
        else:
            return [random.gauss(0,1) for _ in range(self.latent_dim)]

    def generate_rule(self, name_prefix: str = "gen", idx: Optional[int] = None) -> Rule:
        z = self.sample_latent()
        if HAS_NUMPY:
            raw = float(self.W_mean.dot(z) + self.b_mean)
            mean = float(1.0 / (1.0 + math.exp(-raw)))
        else:
            # linear map by pure python
            raw = sum(w*z_i for w,z_i in zip(self.W_mean[0], z)) + self.b_mean[0]
            mean = self._sigmoid(raw)
        # clamp to sensible interval
        mean = max(0.01, min(0.99, mean))
        var = float(self.base_var)
        weight = 1.0
        name = f"{name_prefix}_{idx}" if idx is not None else f"{name_prefix}_{random.randint(0,10**9)}"
        r = Rule(name=name, init_weight=weight, init_mean=mean, init_var=var, latent_z=z)
        self.registry[r.name] = z
        return r

    def train_on_replay(self, replay: ReplayBuffer, rules_map: Dict[str, Rule], lr: float = 0.01, epochs: int = 1, batch: int = 64):
        """Simple supervised gradient step to fit W_mean,b_mean so that sigmoid(Wz+b) ~ observed reward.
        Uses only samples in replay that correspond to generated rules (present in registry).
        """
        if not HAS_NUMPY:
            # no-op if numpy not available
            return
        # collect (z, reward) pairs
        pairs = []
        for (rn, rew) in list(replay.buf):
            if rn in self.registry and rn in rules_map:
                z = np.asarray(self.registry[rn]).reshape(self.latent_dim)
                pairs.append((z, float(rew)))
        if not pairs:
            return
        X = np.vstack([p[0] for p in pairs])
        y = np.asarray([p[1] for p in pairs]).reshape(-1,1)
        # simple gradient descent on logistic output
        for _ in range(epochs):
            # forward
            logits = X.dot(self.W_mean.T) + self.b_mean  # shape (N,1)
            preds = 1.0 / (1.0 + np.exp(-logits))
            error = preds - y  # (N,1)
            grad_W = (error.T.dot(X)) / max(1, X.shape[0])  # (1,D)
            grad_b = np.mean(error, axis=0)  # (1,)
            # gradient step (note W_mean shape (1,D) )
            self.W_mean -= lr * grad_W
            self.b_mean -= lr * grad_b


class DRMSystem:
    def __init__(self):
        self.rules: Dict[str, Rule] = {}
        self.replay = ReplayBuffer(capacity=10000)
        self.generators: List[RuleGenerator] = []

    def register_generator(self, gen: RuleGenerator):
        self.generators.append(gen)

    def add_rule(self, rule: Rule):
        self.rules[rule.name] = rule
        rule.weight = max(EPS, rule.weight)

    def generate_rules(self, generator: RuleGenerator, count: int = 1, name_prefix: str = "gen"):
        for i in range(count):
            r = generator.generate_rule(name_prefix=name_prefix, idx=i)
            self.add_rule(r)

    def get_distribution(self) -> Dict[str, float]:
        names = list(self.rules.keys())
        weights = [max(EPS, self.rules[n].weight) for n in names]
        s = sum(weights) or EPS
        return {n: w / s for n, w in zip(names, weights)}

    def sample_rule_weighted(self) -> Optional[Rule]:
        dist = self.get_distribution()
        if not dist:
            return None
        names = list(dist.keys())
        probs = [dist[n] for n in names]
        choice = random.choices(names, weights=probs, k=1)[0]
        return self.rules[choice]

    def sample_rule_thompson(self) -> Optional[Rule]:
        if not self.rules:
            return None
        best = None
        best_val = -1e9
        for r in self.rules.values():
            if r.quarantined:
                continue
            s = r.sample_posterior()
            if s > best_val:
                best_val = s
                best = r
        return best

    def sample_rule_ucb(self, c: float = 1.0) -> Optional[Rule]:
        total_obs = sum(max(1, r.usage_count) for r in self.rules.values())
        best = None
        best_val = -1e9
        for r in self.rules.values():
            if r.quarantined:
                continue
            bonus = c * math.sqrt(math.log(max(2, total_obs)) / (1 + r.usage_count))
            val = r.post_mean + bonus
            if val > best_val:
                best_val = val
                best = r
        return best

    @staticmethod
    def entropy_from_dist(dist: Dict[str, float]) -> float:
        return -sum(p * math.log(p + EPS) for p in dist.values())

    @staticmethod
    def kl_divergence(p: Dict[str, float], q: Dict[str, float]) -> float:
        keys = set(p.keys()) | set(q.keys())
        kl = 0.0
        for k in keys:
            pk = p.get(k, EPS)
            qk = q.get(k, EPS)
            kl += pk * math.log((pk + EPS) / (qk + EPS))
        return kl

    def compute_scores(self, beta: float = 0.5, lam: float = 0.5, exploration_bonus: float = 0.05) -> Dict[str, float]:
        scores = {}
        for name, r in self.rules.items():
            if r.quarantined:
                scores[name] = -1e9
                continue
            novelty = 1.0 / (1.0 + r.usage_count)
            risk = r.post_var
            exploration = exploration_bonus if r.is_new else 0.0
            scores[name] = float(r.post_mean + beta * novelty - lam * risk + exploration)
        return scores

    def multiplicative_update(self, eta: float = 0.05, beta: float = 0.5, lam: float = 0.5,
                              kl_max: float = 0.5, eta_min: float = 1e-6):
        old_dist = self.get_distribution()
        scores = self.compute_scores(beta=beta, lam=lam)
        names = list(self.rules.keys())

        if HAS_NUMPY:
            # vectorized path
            weights = np.array([max(EPS, self.rules[n].weight) for n in names], dtype=float)
            score_arr = np.array([scores[n] for n in names], dtype=float)
            tentative = weights * np.exp(eta * score_arr)
            if tentative.sum() <= 0:
                tentative = np.maximum(tentative, EPS)
            tentative = tentative / tentative.sum()
            # compute KL
            old_arr = np.array([old_dist[n] for n in names], dtype=float)
            kl = float(np.sum(tentative * np.log((tentative + EPS) / (old_arr + EPS))))
            eta_used = eta
            if kl > kl_max and kl > 0:
                eta_scaled = max(eta_min, eta * (kl_max / kl))
                tentative = weights * np.exp(eta_scaled * score_arr)
                tentative = tentative / tentative.sum()
                kl2 = float(np.sum(tentative * np.log((tentative + EPS) / (old_arr + EPS))))
                back = 0
                while kl2 > kl_max and back < 10:
                    eta_scaled *= 0.5
                    eta_scaled = max(eta_min, eta_scaled)
                    tentative = weights * np.exp(eta_scaled * score_arr)
                    tentative = tentative / tentative.sum()
                    kl2 = float(np.sum(tentative * np.log((tentative + EPS) / (old_arr + EPS))))
                    back += 1
                eta_used = eta_scaled
            # write back scaled to avg_scale
            avg_scale = len(names) or 1
            tentative = tentative * avg_scale
            for i, n in enumerate(names):
                self.rules[n].weight = float(max(EPS, tentative[i]))
            return eta_used
        else:
            # fallback to python loop (previous implementation)
            def _new_weights_for_eta(eta_val):
                nw = {}
                for name, r in self.rules.items():
                    base = max(EPS, r.weight)
                    nw[name] = base * math.exp(eta_val * scores.get(name, 0.0))
                s = sum(nw.values()) or EPS
                return {k: v / s for k, v in nw.items()}
            tentative = _new_weights_for_eta(eta)
            kl = self.kl_divergence(tentative, old_dist)
            if kl > kl_max and kl > 0:
                eta_scaled = max(eta_min, eta * (kl_max / kl))
                tentative = _new_weights_for_eta(eta_scaled)
                kl2 = self.kl_divergence(tentative, old_dist)
                back = 0
                while kl2 > kl_max and back < 10:
                    eta_scaled *= 0.5
                    eta_scaled = max(eta_min, eta_scaled)
                    tentative = _new_weights_for_eta(eta_scaled)
                    kl2 = self.kl_divergence(tentative, old_dist)
                    back += 1
            avg_scale = len(self.rules) or 1
            for name, r in self.rules.items():
                r.weight = max(EPS, tentative.get(name, EPS) * avg_scale)
            return eta

    def detect_dezinformation(self, mu_min: float = 0.1, tau: float = 0.95):
        removed = []
        for name, r in list(self.rules.items()):
            if r.quarantined:
                continue
            prob_below = r.post_prob_below(mu_min)
            if prob_below > tau:
                r.quarantined = True
                removed.append(name)
        return removed

    def run_cycle(self, evaluator,
                  eta: float = 0.05, beta: float = 0.5, lam: float = 0.5,
                  kl_max: float = 0.5, mu_min: float = 0.1, tau: float = 0.95,
                  sampling_strategy: str = "weighted",
                  replay_batch: int = 0,
                  generator_train: bool = False):
        names = list(self.rules.keys())
        if not names:
            return {"status": "no_rules"}
        sampled = []
        for name in names:
            r = self.rules[name]
            reward = evaluator(r)
            if reward is not None:
                self.replay.add((name, float(reward)))
            r.update_posterior(reward)
            sampled.append((name, reward))
        if replay_batch and len(self.replay.buf) > 0:
            replay_samples = self.replay.sample(replay_batch)
            for rn, rew in replay_samples:
                if rn in self.rules:
                    self.rules[rn].update_posterior(rew)
        # optionally train registered generators on replay
        if generator_train and self.generators and len(self.replay.buf) > 0 and HAS_NUMPY:
            for gen in self.generators:
                gen.train_on_replay(self.replay, self.rules, lr=0.01, epochs=1, batch=64)
        # multiplicative update (returns eta_used when numpy path is used)
        eta_used = self.multiplicative_update(eta=eta, beta=beta, lam=lam, kl_max=kl_max)
        quarantined = self.detect_dezinformation(mu_min=mu_min, tau=tau)
        dist_after = self.get_distribution()
        entropy = self.entropy_from_dist(dist_after)
        if sampling_strategy == "thompson":
            selected = self.sample_rule_thompson()
        elif sampling_strategy == "ucb":
            selected = self.sample_rule_ucb()
        else:
            selected = self.sample_rule_weighted()
        selected_name = selected.name if selected is not None else None
        return {
            "evaluations": sampled,
            "quarantined": quarantined,
            "entropy": entropy,
            "distribution": dist_after,
            "selected": selected_name,
            "eta_used": float(eta_used) if isinstance(eta_used, (float, int)) else None,
        }

    def summary(self):
        return {k: self.rules[k].to_dict() for k in self.rules}
